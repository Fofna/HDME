{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a080410",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0673be",
   "metadata": {},
   "source": [
    "By Ladji Idrissa Fofana & Zineb Bouharra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9384a32",
   "metadata": {},
   "source": [
    "### Implications in future researches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad46719",
   "metadata": {},
   "source": [
    "#### Non- Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688f21b",
   "metadata": {},
   "source": [
    "**Theorical Motivations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb9a5d",
   "metadata": {},
   "source": [
    "Matrix factorization is a technique used in collaborative filtering to discover latent features that explain the observed ratings. These latent features, such as action level or family friendliness, can be continuous rather than discrete, allowing the model to capture nuances in individual preferences. This is particularly useful for sparse data, such as movie reviews, where not every user has rated the same movies. Additionally, matrix factorization, specifically Singular Value Decomposition (SVD), can be performed in parallel, making it a good choice for large datasets. In the next steps, we will implement a nonnegative matrix factorization model to improve our recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e479be",
   "metadata": {},
   "source": [
    "The goal is to approximate the original ratings matrix by finding two matrices, $P$ and $Q$, such that the dot product of $P$ and $Q^T$ is close to the original matrix.\n",
    "\n",
    "Let $R$ be a sparse $UxM$ matrix of user-movie ratings, where blank spots represent missing ratings. By factoring $R$ into $P$ and $Q$, with $P$ being of size $UxK$ and $Q$ being of size $MxK$, where $K$ is the number of latent features, we can fill in the missing ratings with predicted values. The intuition is that there are underlying features that influence how users rate items, and by discovering these features we can make better predictions.\n",
    "\n",
    "This task is mathematically represented as: given a set of $U$ users and $M$ movies, find matrices $P$ and $Q$ such that $P$ is $UxK$ and $Q$ is $MxK$, and $PQ^T$ approximates $R$. $P$ represents the strength of the associations between users and latent features, and $Q$ represents the associations of movies with latent features.\n",
    "\n",
    "$$R \\approx P \\times Q^T=\\hat{R}$$\n",
    "\n",
    "The model is generally optimized by $$\\min _{P, Q}\\|R-P Q\\|_F \\quad$$ such that $\\quad P, Q \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a47e5",
   "metadata": {},
   "source": [
    "We used gradient descent to perform matrix factorization by initializing two matrices, $P$ and $Q$, and iteratively adjusting them to minimize the difference between their dot product and the original matrix. This allows the algorithm to find a solution that approximates the original matrix within a certain level of error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1333f3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dba137",
   "metadata": {},
   "source": [
    "Our method for determining the error used in the factorization process is to calculate the square of the difference between the predicted value and the actual value :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f17ba",
   "metadata": {},
   "source": [
    "$$e_{i j}=\\left(r_{i j}-\\hat{r}_{i j}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c49e73",
   "metadata": {},
   "source": [
    "In order to minimize the error, we adjust the values of pik and qkj by following the slope of the error function, which is calculated by taking the derivative of the error equation with respect to p :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7df19e",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial p_{i k}} e_{i j}=-2\\left(r_{i j}-\\hat{r}{i j}\\right)\\left(q_{k j}\\right)=-2 e_{i j} q_{k j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d817c",
   "metadata": {},
   "source": [
    "The error was minimized by adjusting the values of the matrices $p_{k j}$ and $q_{k j}$ by using gradient descent. The gradient of the error equation was calculated by differentiating it with respect to the variables $p$ and $q$, resulting in equations that specify how to adjust the values of $p_{k j}$ and $q_{k j}$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80fd48",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial q_{i k}} e_{i j}=-2\\left(r_{i j}-\\hat{r}{i j}\\right)\\left(p_{i k}\\right)=-2 e_{i j} p_{i k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d106cd",
   "metadata": {},
   "source": [
    "The values of matrices $P$ and $Q$ are updated according to the learning rule, which is determined by a constant learning rate $\\alpha$. This rate should be chosen carefully, as a too large value may cause the algorithm to step over the minimum, while a too small value may result in slow convergence :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3867bcc",
   "metadata": {},
   "source": [
    "$$p_{i k}^{\\prime}=p_{i k}+\\alpha \\frac{\\partial}{\\partial p_{i k}} e_{i j}=p_{i k}+2 \\alpha e_{i j} q_{k j}$$\n",
    "\\\n",
    "$$q_{k j}^{\\prime}=q_{k j}+\\alpha \\frac{\\partial}{\\partial q_{k j}} e_{i j}=q_{k j}+2 \\alpha e_{i j} p_{i k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d5d66",
   "metadata": {},
   "source": [
    "We can stop the iterative process of updating the matrices P and Q when the sum of the error squared falls below a certain threshold value or after a certain number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca097491",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5aa31",
   "metadata": {},
   "source": [
    " The purpose of matrix factorization is to decompose a given matrix $R$ into two matrices $P$ and $Q$, where $P$ represents the user-feature matrix and $Q$ represents the item-feature matrix.After using the __gradient descent algorithm__, the product of these two matrices approximates the original matrix $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2049f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize(R, K):\n",
    "    \"\"\"\n",
    "    Returns initial matrices for an N X M matrix, R and K features.\n",
    "\n",
    "     R: the matrix to be factorized\n",
    "     K: the number of latent features\n",
    "\n",
    "    :returns: P, Q initial matrices of N x K and M x K sizes\n",
    "    \"\"\"\n",
    "    N, M = R.shape # Get the shape of the input matrix R\n",
    "    P = np.random.rand(N,K) # Initialize the matrix P with random values between 0 and 1 of size N x K\n",
    "    Q = np.random.rand(M,K) # Initialize the matrix Q with random values between 0 and 1 of size M x K\n",
    "\n",
    "    return P, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc6f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF(R, P=None, Q=None, K=2, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    \"\"\"\n",
    "    Performs matrix factorization on R with given parameters.\n",
    "\n",
    "     R: A matrix to be factorized, dimension N x M\n",
    "     P: an initial matrix of dimension N x K\n",
    "     Q: an initial matrix of dimension M x K\n",
    "     K: the number of latent features\n",
    "     steps: the maximum number of iterations to optimize in\n",
    "     alpha: the learning rate for gradient descent\n",
    "     beta:  the regularization parameter\n",
    "\n",
    "    :returns: final matrices P and Q\n",
    "    \"\"\"\n",
    "\n",
    "    if not P or not Q:\n",
    "        P, Q = initialize(R, K) #initialize P and Q if they are not provided\n",
    "    Q = Q.T # transpose Q\n",
    "\n",
    "    rows, cols = R.shape\n",
    "    for step in range(steps): # loop through the maximum number of iterations to optimize\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if R[i,j] > 0:\n",
    "                    eij = R[i,j] - np.dot(P[i,:], Q[:,j]) # calculate the error for R[i,j]\n",
    "                    for k in range(K):\n",
    "                        P[i,k] = P[i,k] + alpha * (2 * eij * Q[k,j] - beta * P[i,k]) # update P[i,k]\n",
    "                        Q[k,j] = Q[k,j] + alpha * (2 * eij * P[i,k] - beta * Q[k,j]) # update Q[k,j]\n",
    "\n",
    "        e  = 0\n",
    "        for i in range(rows): # loop through the rows of the matrix R\n",
    "            for j in range(cols): # loop through the columns of the matrix R\n",
    "                if R[i,j] > 0:\n",
    "                    e = e + pow(R[i,j] - np.dot(P[i,:], Q[:,j]), 2) # calculate the total error\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * (pow(P[i,k], 2) + pow(Q[k,j], 2)) # regularization term\n",
    "        if e < 0.001:\n",
    "            break\n",
    "\n",
    "    return P, Q.T # return the final matrices P and Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76ec75",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6724e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix P:\n",
      "[[ 0.26263117  0.97159489  0.83697281  0.63844373  0.61396705]\n",
      " [ 0.30412048  0.66432076  0.23501567  0.14256753  0.81771627]\n",
      " [ 1.07023729  0.66152944  0.53657825  0.45153353  0.74706731]\n",
      " [ 0.3499086   0.43245018  0.05014987  0.62063163  0.53055552]\n",
      " [ 0.94392242  0.13486212 -0.02149571  0.28769792  0.86313328]]\n",
      "Matrix Q:\n",
      "[[ 1.21815326 -0.02950767  0.0745381   0.36455737  0.71446784]\n",
      " [ 0.52777883  0.51331496  0.78883184  0.56871233  0.47260434]\n",
      " [ 0.31828754  0.33330419  0.00748847  0.02085077  0.34487079]\n",
      " [ 0.42513636  0.07962145  0.26457821  0.56286521  0.79233743]]\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "R = np.array([[1,2,0,0], [1,0,0,0], [2,2,0,0], [1,0,0,1], [2,1,0,0]])\n",
    "\n",
    "# Call the factor function\n",
    "P, Q = NMF(R, K=5, steps=5000, alpha=0.0002, beta=0.02)\n",
    "\n",
    "# Print the results\n",
    "print(\"Matrix P:\")\n",
    "print(P)\n",
    "print(\"Matrix Q:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fda387",
   "metadata": {},
   "source": [
    "#### NMF- ALSWR - Projected Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4cd3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nmf_als_wr(R, k, l1, l2, tol, max_iter):\n",
    "    n, m = R.shape  # get the dimensions of the input matrix R\n",
    "    M = np.random.rand(n, k)  # initialize the matrix M with random values\n",
    "    U = np.random.rand(k, m)  # initialize the matrix U with random values\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # solve for U using least squares, with a regularization term added\n",
    "        U = np.linalg.solve(M.T @ M + l2 * np.eye(k), M.T @ R)\n",
    "        U = np.maximum(U, 0)  # project U back into the non-negative space\n",
    "        \n",
    "        # solve for M using least squares, with a regularization term added\n",
    "        M = np.linalg.solve(U @ U.T + l2 * np.eye(k), R @ U.T)\n",
    "        M = np.maximum(M, 0)  # project M back into the non-negative space\n",
    "        \n",
    "        # compute the L2-norm weight regularization term\n",
    "        M_proj = M / np.sqrt(np.sum(M**2, axis=0))\n",
    "        M_proj = M_proj * np.minimum(1, np.sqrt(l1 / np.sum(M_proj**2, axis=0)))\n",
    "        M = M_proj\n",
    "        \n",
    "        # compute the reconstruction error\n",
    "        error = np.linalg.norm(R - M @ U)\n",
    "        if error < tol:  # check if the error is below the tolerance\n",
    "            break\n",
    "    \n",
    "    return M, U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50189c",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d86635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\n",
      "[[0.         0.        ]\n",
      " [0.31622777 0.31622777]]\n",
      "U:\n",
      "[[3.16227766 3.95284708 4.74341649]\n",
      " [3.16227766 3.95284708 4.74341649]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input matrix R\n",
    "R = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Define the number of components k\n",
    "k = 2\n",
    "\n",
    "# Define the regularization parameters l1 and l2\n",
    "l1 = 0.1\n",
    "l2 = 0.2\n",
    "\n",
    "# Define the tolerance tol\n",
    "tol = 1e-6\n",
    "\n",
    "# Define the maximum number of iterations\n",
    "max_iter = 100\n",
    "\n",
    "# Call the nmf_als_wr function\n",
    "M, U = nmf_als_wr(R, k, l1, l2, tol, max_iter)\n",
    "\n",
    "# Print the results\n",
    "print(\"M:\")\n",
    "print(M)\n",
    "print(\"U:\")\n",
    "print(U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571428af",
   "metadata": {},
   "source": [
    "#### An Alternative NMF without Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49622460",
   "metadata": {},
   "source": [
    "The input to the algorithm is a non-negative matrix $R$, the number of components (or rank) of the matrices being decomposed, and optional parameters for the maximum number of iterations and the tolerance for approximation error.\n",
    "\n",
    "The algorithm initializes two matrices $P$ and $Q$ with random non-negative values, and then iteratively updates them using the multiplicative update rule. Specifically, in each iteration:\n",
    "\n",
    "The matrix $Q$ is updated by element-wise multiplication with the term $(P^T R) / (P^T P Q)$\n",
    "The matrix $Q$ is then thresholded to ensure that all its elements are non-negative\n",
    "The matrix $P$ is updated by element-wise multiplication with the term $(R Q^T) / (P Q Q^T)$\n",
    "The matrix $P$ is then thresholded to ensure that all its elements are non-negative\n",
    "The approximation error is computed as the Frobenius norm of the difference between the original matrix $R$ and the product of the two matrices $P$ and $Q$. If the error is below the given tolerance, the algorithm terminates. Otherwise, the algorithm iterates until the maximum number of iterations is reached.\n",
    "\n",
    "The output of the algorithm is the two matrices $P$ and $Q$ that approximate the original matrix $R$. NMF can be used for various applications such as data compression, feature extraction, and topic modeling.\n",
    "\n",
    "\n",
    "$$ R ≈ P×Q $$\n",
    "\n",
    "Where P and Q are the non-negative matrices.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71cdd0",
   "metadata": {},
   "source": [
    "#### NMF Using Sckit- Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dbb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.45619443  0.53202653  0.19429266 -1.3950989  -1.45446554]\n",
      " [-1.58134923 -0.64240736 -0.15577295  1.53037729  1.54955308]\n",
      " [-0.82842747  1.06464528 -0.53986046 -0.13514694  1.21241493]\n",
      " [-0.54048483  0.20000553  0.23826174 -0.12478481  0.50938129]\n",
      " [ 1.57977729 -0.96161356  0.29399509 -0.12588792 -1.87985208]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simulated matrix\n",
    "np.random.seed(0)\n",
    "matrix = np.random.randint(10, size=(5, 5))\n",
    "\n",
    "# Initialize the NMF model\n",
    "nmf_model = NMF(n_components=2)\n",
    "\n",
    "# Fit the model to the matrix\n",
    "nmf_model.fit(matrix)\n",
    "\n",
    "# Get the factorization matrices\n",
    "W = nmf_model.transform(matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Reconstruction of the matrix\n",
    "reconstructed_matrix = np.dot(W, H)\n",
    "\n",
    "print(reconstructed_matrix - matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8162ca",
   "metadata": {},
   "source": [
    "### Practice Using Netflix Competition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cccabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1cad316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n"
     ]
    }
   ],
   "source": [
    "# Import text data into an RDD\n",
    "small_ratings_raw_data = spark.sparkContext.textFile(\"ratings.csv\")\n",
    "# Identify and display the first line\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "print(small_ratings_raw_data_header)\n",
    "# Create RDD without header\n",
    "all_lines = small_ratings_raw_data.filter(lambda l : l!=small_ratings_raw_data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb625434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user=1, item=1, rating=4.0, timestamp=964982703),\n",
       " Row(user=1, item=3, rating=4.0, timestamp=964981247)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the fields (user, item, rating) into a new RDD.\n",
    "from pyspark.sql import Row\n",
    "split_lines = all_lines.map(lambda l : l.split(\",\"))\n",
    "ratingsRDD = split_lines.map(lambda p: Row(user=int(p[0]), item=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# .cache(): the RDD is kept in memory once processed\n",
    "ratingsRDD.cache()\n",
    "\n",
    "# Display the two first rows\n",
    "ratingsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "100fea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "ratingsDF = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69a19a",
   "metadata": {},
   "source": [
    "### Rank optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a301320",
   "metadata": {},
   "source": [
    "The file contains 10,000 ratings cross-referencing the opinions of a thousand users on the movies they have seen among 1700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4bb3c",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "Random separation into three samples learning, validation and testing. The rank parameter is optimized by minimizing the error estimate on the test sample. This strategy, rather than cross-validation, is more suited to big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1203204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxTrain=0.6\n",
    "tauxVal=0.2\n",
    "tauxTes=0.2\n",
    "# If the total is less than 1, the data is undersampled.\n",
    "(trainDF, validDF, testDF) = ratingsDF.randomSplit([tauxTrain, tauxVal, tauxTes])\n",
    "# Validation and testing to predict\n",
    "validDF_P = validDF.select(\"user\", \"item\")\n",
    "testDF_P = testDF.select(\"user\", \"item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346390e",
   "metadata": {},
   "source": [
    "#### NMF rank optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ec628",
   "metadata": {},
   "source": [
    "The error of imputation of the data, therefore of recommendation, is estimated on the validation sample for different values (grid) of the rank of the matrix factorization.\n",
    "\n",
    "In principle, the value of the penalty parameter should also be optimised at 0.1 by default.\n",
    "\n",
    "Important point: the factorization fit error only takes into account the values listed in the hollow matrix, not the \"0s\" which are missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26bf861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import math\n",
    "import collections\n",
    "# Set the seed\n",
    "seed = 5\n",
    "#Number of max iteration for ALS\n",
    "maxIter = 10\n",
    "# L1 Regularization; also to be optimized\n",
    "regularization_parameter = 0.1\n",
    "# Choice of a grid for optimizing rank values\n",
    "ranks = [4, 8, 12]\n",
    "#Initializing variable\n",
    "# Creating a dictionary to store the error by tested rank\n",
    "errors = collections.defaultdict(float)\n",
    "tolerance = 0.02\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104e9876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for rank 4 = 1.1742020059451628\n",
      "Root-mean-square error for rank 8 = 1.1771770762246312\n",
      "Root-mean-square error for rank 12 = 1.1776044891236213\n",
      "Rang optimal: 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "for rank in ranks:\n",
    "    als = ALS( rank=rank, seed=seed, maxIter=maxIter,\n",
    "                      regParam=regularization_parameter)\n",
    "    model = als.fit(trainDF)\n",
    "    # Validation Sample Forecast\n",
    "    predDF = model.transform(validDF).select(\"prediction\",\"rating\")\n",
    "    #Remove unpredicter row due to no-presence of user in the train dataset\n",
    "    pred_without_naDF = predDF.na.fill(0)\n",
    "    # Compute the RMSE\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(pred_without_naDF)\n",
    "    print(\"Root-mean-square error for rank %d = \"%rank + str(rmse))\n",
    "    errors[rank] = rmse\n",
    "    if rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_rank = rank\n",
    "# Best result\n",
    "print('Rang optimal: %s' % best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16eaf5",
   "metadata": {},
   "source": [
    "#### Final prediction of the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8f7e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for rank 4 = 0.5352747677834642\n"
     ]
    }
   ],
   "source": [
    "# We concatenate here the Train and Validation DataFrames.\n",
    "trainValidDF = trainDF.union(validDF)\n",
    "# We create a model with the new completed training Dataframe and the rank set to the optimal value.\n",
    "als = ALS( rank=best_rank, seed=seed, maxIter=maxIter,\n",
    "                  regParam=regularization_parameter)\n",
    "model = als.fit(trainValidDF)\n",
    "# Predicting on the Test DataFrame\n",
    "testDF = model.transform(testDF).select(\"prediction\",\"rating\")\n",
    "#Remove unpredicter row due to no-presence of user in the trai dataset\n",
    "pred_without_naDF = predDF.na.fill(0)\n",
    "# Calcul du RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                            predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred_without_naDF)\n",
    "print(\"Root-mean-square error for rank %d = \"%best_rank + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280baedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He we Unzip downloaded file which contains our matrix data\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(\"ml-ratings20M.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cabf55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the text data into an RDD.\n",
    "ratings_raw_data = spark.sparkContext.textFile(\"ratings20M.csv\")\n",
    "\n",
    "# Identify and display the first line\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_raw_data_header\n",
    "\n",
    "# We now create RDD without header\n",
    "all_lines = ratings_raw_data.filter(lambda l : l!=ratings_raw_data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "417d2f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user=1, item=169, rating=2.5, timestamp=1204927694),\n",
       " Row(user=1, item=2471, rating=3.0, timestamp=1204927438)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the fields (user, item, rating) into a new RDD\n",
    "split_lines = all_lines.map(lambda l : l.split(\",\"))\n",
    "ratingsRDD = split_lines.map(lambda p: Row(user=int(p[0]), item=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# Display the two first rows\n",
    "ratingsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7a934c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "ratingsDF = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c034db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxTest=0.1\n",
    "# If the total is less than 1, the data is undersampled.\n",
    "(trainTotDF,  testDF) = ratingsDF.randomSplit([1-tauxTest, tauxTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "581d648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling learning to test for increasing sizes of this sample.\n",
    "tauxEch=0.2\n",
    "(trainDF, DropData) = trainTotDF.randomSplit([tauxEch, 1-tauxEch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b31943",
   "metadata": {},
   "source": [
    "#### Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee80e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS prend 126 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_start=time.time()\n",
    "# Set the seed\n",
    "seed = 5\n",
    "# Max number of itertion for ALS\n",
    "maxIter = 10\n",
    "# L1 penalization\n",
    "regularization_parameter = 0.1\n",
    "best_rank = 8\n",
    "# Estimate for each rank value\n",
    "als = ALS(rank=rank, seed=seed, maxIter=maxIter,\n",
    "                      regParam=regularization_parameter)\n",
    "model = als.fit(trainDF)\n",
    "time_end=time.time()\n",
    "time_als=(time_end - time_start)\n",
    "print(\"ALS prend %d s\" %(time_als))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dcc0cf",
   "metadata": {},
   "source": [
    "#### Prediction Error on the Test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e644b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error for rank 8 = 0.5295778529450182\n"
     ]
    }
   ],
   "source": [
    "# Sample Validation Forecast\n",
    "predDF = model.transform(testDF).select(\"prediction\",\"rating\")\n",
    "#Remove unpredicter row due to no-presence of user in the train dataset\n",
    "pred_without_naDF = predDF.na.fill(0)\n",
    "# Compute RMSE \n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                            predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred_without_naDF)\n",
    "print(\"Root-mean-square error for rank %d = \"%best_rank + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af63598",
   "metadata": {},
   "source": [
    "### Generalisation to Tensor Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59578163",
   "metadata": {},
   "source": [
    "<img src=\"Tensor.png\" width=\"350\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72716bc6",
   "metadata": {},
   "source": [
    "We consider here the dataset as a tensor where the first dimension is users, the second dimension is movies, and the third dimension is ratings. In general, any multi-dimensional array can be considered a tensor, and **NTF** can be applied to decompose it into a sum of rank-1 tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86037a1",
   "metadata": {},
   "source": [
    "Non-negative tensor factorization (NTF) is a method that decomposes a non-negative tensor (a multi-dimensional array with non-negative entries) into a sum of rank-1 non-negative tensors. It can be used for various tasks such as image and video analysis, data mining, and recommendation systems. In the context of a movie recommendation system, NTF can be used to factorize a user-movie-rating tensor into a user-feature matrix, a movie-feature matrix, and a feature-rating matrix. These matrices can then be used to predict missing ratings, recommend movies to users, and provide insights into the characteristics of users and movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec2887",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
